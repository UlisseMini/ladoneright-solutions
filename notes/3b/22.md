Suppose $U$ and $V$ are finite-dimensional vector spaces and $S \in \mathcal L(V,W)$ and $T \in \mathcal L(U,V)$. Prove that
$$
\dim \text{null }ST \le \dim \text{null }S + \dim \text{null }T
$$

---

Let $u_1,\dots,u_r$ be a basis for $\text{null }T$, since $\text{null }T \subseteq \text{null }ST$ we can extend it to a basis $u_1,\dots,u_m$ of $\text{null }ST$. Let $R = \text{span}(u_{r+1},\dots,u_m)$. $T|_R$ ($T$ restricted to $R$) is injective since
$$
\text{null }T|_R = \text{null($T$)} \cap R = \text{span}(u_1,\dots,u_r) \cap \text{span}(u_{r+1},\dots,u_m) = \{0\}
$$
Therefor $Tu_{r+1},\dots,Tu_m$ are linearly independent in $V$ meaning we can extend it to a basis of $V$
$$
Tu_{r+1},\dots,Tu_m,v_1,\dots,v_n
$$
which implies $\dim \text{null }ST \le \dim \text{null }S + \dim \text{null }T$ since we constructed a basis for $\text{null }S$ and $\text{null }T$ by extending a basis of $\text{null }ST$.

---

This was intuitively obvious but annoying to prove rigorously.
